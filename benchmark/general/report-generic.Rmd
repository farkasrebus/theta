---
title: "Benchmark report"
params:
  csv_path: "test.csv"
  timeout_ms: 10000
output:
  html_document:
    code_folding: hide
    number_sections: yes
    toc: no
    toc_depth: 2
  pdf_document:
    highlight: zenburn
    number_sections: yes
    toc: no
    toc_depth: 2
classoption: a4paper
---

This is a benchmark report for the file `r params$csv_path`.

# Initialization

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(forcats)
library(lubridate)
library(corrgram)
```

```{r}
# Themes
theme_rotate_x <- theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
theme_noticks <- theme(axis.ticks = element_blank())
```

## Load data

```{r}
d <- read_csv(
  params$csv_path,
  col_types = cols(
    Model = col_character(),
    Config = col_character(),
    Safe = col_character(),
    TimeMs = col_integer()
  )
)
```

## Clean data

```{r}
# Convert 'Safe' to logical (and remove exceptions).
empty_rows <- sum(is.na(d$Safe))
if (empty_rows > 0) warning(paste(c(empty_rows, " rows contain missing values (no timeout, no exception)")))
ex_rows <- str_detect(d$Safe, "\\[EX\\]")
exs <- sum(ex_rows, na.rm = T)
if (exs > 0) warning(paste(c(exs, " rows contain exceptions that are converted to NAs.")))
d$Safe <- as.logical(d$Safe)
```

# Overview

## Summary

```{r}
n_meas_total <- nrow(d)
n_meas_succ <- nrow(d %>% filter(!is.na(Safe)))
n_models_total <- length(unique(d$Model))
n_configs_total <- length(unique(d$Config))
total_time <- seconds_to_period(sum(ifelse(is.na(d$TimeMs), params$timeout_ms, d$TimeMs)) / 1000)

d_model_config <- d %>%
  group_by(Config, Model) %>%
  summarise(
    ResultCount = sum(!is.na(Safe)),
    Succ = ResultCount > 0,
    SafeCount = sum(Safe, na.rm = T),
    Consistent = SafeCount == 0 || SafeCount == ResultCount,
    TimeAvg = mean(TimeMs),
    TimeRSD = ifelse(ResultCount == 1, 0, sd(TimeMs) / mean(TimeMs)))
d_model <- d_model_config %>% group_by(Model) %>%
  summarise(
    ResultCount = sum(ResultCount),
    Succ = ResultCount > 0,
    SafeCount = sum(SafeCount),
    Consistent = SafeCount == 0 || SafeCount == ResultCount,
    TimeMin = min(TimeAvg, na.rm = T),
    ConfigMin = ifelse(is.na(TimeMin), "", Config[which.min(TimeAvg)]))
d_model$ConfigMin[d_model$ConfigMin == ""] <- NA

n_models_succ <- length(unique((d_model %>% filter(Succ))$Model))
n_configs_succ <- length(unique((d_model_config %>% filter(Succ))$Config))
```

- There are **`r n_models_total` models** and **`r n_configs_total` configurations**, giving **`r n_models_total * n_configs_total` measurements**, with **`r nrow(d_model_config %>% filter(Succ))` being successful** with the timeout limit of `r params$timeout_ms/1000` s (`r nrow(d_model_config %>% filter(Succ)) / (n_models_total * n_configs_total)` success rate).
    - **`r n_models_succ` / `r n_models_total` models** were verified by at least one configuration.
    - **`r n_configs_succ` / `r n_configs_total` configurations** verified at least one model.
- With the repeated measurements included, there are a total number of **`r n_meas_total` measurement points** with **`r n_meas_succ` successful** executions.
- Total time of the measurements is **`r total_time$day` days, `r total_time$hour` hours and `r total_time$minute` minutes**, assuming that the missing values timed out.

## Consistency of results

There are **`r sum(!d_model_config$Consistent)`** cases where different executions of the same configuration yielded different results for the same model.
```{r}
d_model_config %>% filter(!Consistent) %>% ungroup() %>% select(Config, Model)
```

There are **`r sum(!d_model$Consistent)`** cases where different executions of the same configuration or different configurations yielded different results for the same model.
```{r}
d_model %>% filter(!Consistent) %>% select(Model)
```

## Overview of models and configurations

```{r, fig.height = 4}
ggplot(d_model_config) +
  geom_bar(aes(x = Model, fill = Succ)) +
  scale_x_discrete(drop = F) +
  theme_rotate_x +
  labs(title = "Number of configurations that verified a model")
```

```{r, fig.height = 4}
ggplot(d_model_config) +
  geom_bar(aes(x = Config, fill = Succ)) +
  scale_x_discrete(drop = F) +
  theme_rotate_x +
  labs(title = "Number of models verified by a configuration")
```

```{r}
ggplot(d_model_config, aes(Config, Model)) +
  geom_tile(aes(fill = ResultCount), color = "black") +
  scale_fill_gradient(low = "red", high = "green", na.value = "white") +
  theme_noticks + theme_rotate_x +
  labs(title = "Number of successful executions")
```

# Details

## Execution time

### Relative Standard Deviation

```{r, fig.height = 2}
ggplot(d_model_config) +
  geom_histogram(aes(TimeRSD)) +
  labs(title = "Overall distribution of the RSD")
```

The maximum RSD is **`r max(d_model_config$TimeRSD, na.rm = T)`**, and 95% of the measurements have a lower RSD than `r quantile(d_model_config$TimeRSD, .95, na.rm = T)`.

```{r}
ggplot(d_model_config, aes(Config, Model)) +
  geom_tile(aes(fill = TimeRSD), color = "black") +
  scale_fill_gradient(low = "green", high = "red", na.value = "white") +
  theme_noticks + theme_rotate_x +
  labs(title = "Individual RSD for each configuration and model")
```

### Average execution time

```{r, fig.height = 3}
ggplot(d %>% filter(!is.na(TimeMs))) +
  geom_histogram(aes(TimeMs)) +
  labs(title = "Distribution of execution time")
summary(d$TimeMs)
```


```{r}
ggplot(d %>% filter(!is.na(TimeMs))) +
  geom_boxplot(aes(Model, log10(TimeMs))) + coord_flip() +
  labs(title = "Distribution of execution time for each model")
```

```{r}
ggplot(d_model_config, aes(Config, Model)) +
  geom_tile(aes(fill = log10(TimeAvg)), color = "black") +
  scale_fill_gradient(low = "green", high = "red", na.value = "white") +
  theme_noticks + theme_rotate_x +
  labs(title = "Heatmap of execution time for each model/configuration")
```

```{r, fig.height = 4}
ggplot(d_model %>% filter(!is.na(ConfigMin))) +
  geom_bar(aes(x = ConfigMin)) +
  theme_rotate_x +
  labs(title = "Number of models where a configuration was the fastest")
```